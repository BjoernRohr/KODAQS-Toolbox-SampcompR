@Article{rohrComparingAccuracyUnivariate2024a,
  title = {Comparing the {{Accuracy}} of {{Univariate}}, {{Bivariate}}, and {{Multivariate Estimates}} across {{Probability}} and {{Nonprobability Surveys}} with {{Population Benchmarks}}},
  author = {Bj{\"o}rn Rohr and Henning Silber and Barbara Felderer},
  year = {2024},
  month = {sep},
  journal = {Sociological Methodology},
  pages = {00811750241280963},
  publisher = {SAGE Publications Inc},
  issn = {0081-1750},
  doi = {10.1177/00811750241280963},
  urldate = {2024-10-17},
  abstract = {Previous studies have shown many instances where nonprobability surveys were not as accurate as probability surveys. However, because of their cost advantages, nonprobability surveys are widely used, and there is much debate over the appropriate settings for their use. To contribute to this debate, we evaluate the accuracy of nonprobability surveys by investigating the common claim that estimates of relationships are more robust to sample bias than means or proportions. We compare demographic, attitudinal, and behavioral variables across eight German probability and nonprobability surveys with demographic and political benchmarks from the microcensus and a high-quality, face-to-face survey. In the analyses, we compare three types of statistical inference: univariate estimates, bivariate Pearson's r coefficients, and 24 different multiple regression models. The results indicate that in univariate comparisons, nonprobability surveys were clearly less accurate than probability surveys when compared with the population benchmarks. These differences in accuracy were smaller in the bivariate and the multivariate comparisons across surveys. In addition, the outcome of those comparisons largely depended on the variables included in the estimation. The observed sample differences are remarkable when considering that three nonprobability surveys were drawn from the same online panel. Adjusting the nonprobability surveys somewhat improved their accuracy.},
  langid = {english},
  file = {C:\Users\rohrbn\Zotero\storage\HAUHN85C\Rohr et al. - 2024 - Comparing the Accuracy of Univariate, Bivariate, and Multivariate Estimates across Probability and N.pdf},
}

@Manual{SheaWooldridge,
    title = {wooldridge: 115 Data Sets from "Introductory Econometrics: A Modern
Approach, 7e" by Jeffrey M. Wooldridge},
    author = {Justin M. Shea},
    year = {2024},
    note = {R package version 1.4-4},
    url = {https://CRAN.R-project.org/package=wooldridge},
  }


@article{grovesTotalSurveyError2010,
	title = {Total {Survey} {Error}: {Past}, {Present}, and {Future}},
	volume = {74},
	issn = {0033-362X, 1537-5331},
	shorttitle = {Total {Survey} {Error}},
	url = {https://academic.oup.com/poq/article-lookup/doi/10.1093/poq/nfq065},
	doi = {10.1093/poq/nfq065},
	abstract = {Total survey error’’ is a conceptual framework describing statistical error properties of sample survey statistics. Early in the history of sample surveys, it arose as a tool to focus on implications of various gaps between the conditions under which probability samples yielded unbiased estimates of ﬁnite population parameters and practical situations in implementing survey design. While the framework permits design-based estimates of various error components, many of the design burdens to produce those estimates are large, and in practice most surveys do not implement them. Further, the framework does not incorporate other, nonstatistical, dimensions of quality that are commonly utilized in evaluating statistical information. The importation of new modeling tools brings new promise to measuring total survey error components, but also new challenges. A lasting value of the total survey error framework is at the design stage of a survey, to attempt a balance of costs and various errors. Indeed, this framework is the central organizing structure of the ﬁeld of survey methodology.},
	language = {en},
	number = {5},
	urldate = {2022-08-10},
	journal = {Public Opinion Quarterly},
	author = {Groves, R. M. and Lyberg, L.},
	month = jan,
	year = {2010},
	pages = {849--879},
	file = {Groves und Lyberg - 2010 - Total Survey Error Past, Present, and Future.pdf:C\:\\Users\\rohrbn\\Zotero\\storage\\G78Q9DHJ\\Groves und Lyberg - 2010 - Total Survey Error Past, Present, and Future.pdf:application/pdf},
}

@techreport{mcpheeDataQualityMetrics2022,
	type = {{AAPOR} {Task} {Force} {Report}},
	title = {Data {Quality} {Metrics} for {Online} {Samples}: {Considerations} for {Study} {Design} and {Analysis}},
	url = {https://aapor.org/wp-content/uploads/2023/02/Task-Force-Report-FINAL.pdf},
	language = {en},
	urldate = {2023-10-16},
	author = {McPhee, Cameron and Barlas, Frances and Brigham, Nancy and Darling, Jill and Dutwin, David and Jackson, Chris and Jackson, Mickey and Kirzinger, Ashley and Little, Roderick and Lorenz, Emily and Marlar, Jenny and Mercer, Andrew and Scanlon, Paul J and Weiss, Steffen and Wronski, Laura},
	year = {2022},
	file = {McPhee et al. - Data Quality Metrics for Online Samples Considera.pdf:C\:\\Users\\rohrbn\\Zotero\\storage\\KV3P6CWR\\McPhee et al. - Data Quality Metrics for Online Samples Considera.pdf:application/pdf},
}

@book{rohrSampcompRComparingVisualizing2024,
	title = {{sampcompR}: {Comparing} and {Visualizing} {Differences} {Between} {Surveys}},
	url = {https://CRAN.R-project.org/package=sampcompR},
	author = {Rohr, Bjoern and Felderer, Barbara},
	year = {2024},
}
